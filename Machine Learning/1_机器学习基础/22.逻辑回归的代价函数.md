好的，我们来深入讲解逻辑回归的代价函数。这是理解逻辑回归如何“学习”的关键所在。

---

### 1. 为什么需要专门的代价函数？

首先，我们来回想一下线性回归的代价函数——**均方误差**。它的形式是 `(1/2m) * Σ (y_pred - y_true)²`。它在线性回归中工作得很好，因为输出是连续的。

但在逻辑回归中，我们的输出是经过Sigmoid函数压缩后的**概率** `ŷ = σ(z)`，其值域在(0, 1)之间。如果我们依然使用均方误差作为代价函数，会带来一个问题：

**非凸优化问题**
均方误差应用在逻辑回归的Sigmoid输出上，会形成一个“非凸”的代价函数曲线。这意味着函数的图形会有多个“坑洼”（局部最小值），而不是一个光滑的“碗状”（全局最小值）。梯度下降算法很可能会陷入一个局部最小值而无法找到全局最优解，从而导致模型性能不佳。



因此，我们需要一个能为逻辑回归带来**凸性**（只有一个最小值）的代价函数，以确保梯度下降能找到全局最优解。

---

### 2. 逻辑回归代价函数的直观推导

我们可以从理想情况出发来构建这个函数。我们希望：
*   当真实标签 `y = 1` 时，模型的预测概率 `ŷ` **越大越好**。如果预测不准（`ŷ` 很小），代价就应该**很高**。
*   当真实标签 `y = 0` 时，模型的预测概率 `ŷ` **越小越好**。如果预测不准（`ŷ` 很大），代价就应该**很高**。

我们可以用以下两个曲线来分别满足这两种情况：

*   **当 y = 1 时**：`Cost = -log(ŷ)`
    *   如果 `ŷ = 1`（预测完全正确），`Cost = -log(1) = 0`。
    *   如果 `ŷ -> 0`（预测完全错误），`Cost = -log(0) -> +∞`。
    *   直观理解：预测得越错，代价就趋近于无穷大。



*   **当 y = 0 时**：`Cost = -log(1 - ŷ)`
    *   如果 `ŷ = 0`（预测完全正确），`Cost = -log(1) = 0`。
    *   如果 `ŷ -> 1`（预测完全错误），`Cost = -log(0) -> +∞`。
    *   直观理解：同样，预测得越错，代价就趋近于无穷大。



现在，我们将这两个情况合并成一个优雅的公式：

### 3. 逻辑回归的代价函数：交叉熵损失

**单个样本的损失函数：**

**Loss(ŷ, y) = - [ y * log(ŷ) + (1 - y) * log(1 - ŷ) ]**

让我们来验证一下这个公式的巧妙之处：
*   当 **y = 1** 时，公式后半部分 `(1-y)log(1-ŷ)` 为0，损失函数变为 `-log(ŷ)`，与我们上面的设计一致。
*   当 **y = 0** 时，公式前半部分 `y*log(ŷ)` 为0，损失函数变为 `-log(1 - ŷ)`，也与我们上面的设计一致。

**整个训练集的代价函数：**
我们对所有训练样本（m个）的损失求平均，就得到了全局的代价函数：

**J(w, b) = -(1/m) * Σ [ y⁽ⁱ⁾ * log(ŷ⁽ⁱ⁾) + (1 - y⁽ⁱ⁾) * log(1 - ŷ⁽ⁱ⁾) ]**

其中：
*   `m` 是训练样本的数量。
*   `y⁽ⁱ⁾` 是第 `i` 个样本的真实标签（0或1）。
*   `ŷ⁽ⁱ⁾` 是第 `i` 个样本的预测概率，即 `ŷ⁽ⁱ⁾ = σ(z⁽ⁱ⁾) = σ(wᵀx⁽ⁱ⁾ + b)`。
*   `Σ` 表示对所有训练样本求和。

这个代价函数也被称为**交叉熵损失函数**，它衡量的是两个概率分布（真实分布 `y` 和预测分布 `ŷ`）之间的差异。

---

### 4. 为什么这个代价函数是有效的？（凸性证明）

虽然严格的数学证明比较复杂，但我们可以直观地理解为什么交叉熵损失是凸的：

*   **对数函数 `log(x)` 是凹函数**。
*   在损失函数 `-log(ŷ)` 和 `-log(1-ŷ)` 中，`ŷ` 本身是参数 `w` 和 `b` 通过Sigmoid函数得到的复杂函数。
*   关键在于，**Sigmoid函数和交叉熵损失的组合**，恰好消除了各自的“非线性”和“凹性”，最终得到一个关于参数 `w` 和 `b` 的**凸函数**。
*   这意味着代价函数 `J(w, b)` 就像一个光滑的碗，没有局部最小值，只有唯一的一个全局最小值。这使得梯度下降可以安全、可靠地找到最优解。

---

### 5. 梯度下降的应用

有了凸的代价函数，我们就可以使用梯度下降来优化参数 `w` 和 `b`了。

我们需要计算代价函数 `J(w, b)` 关于每个参数的偏导数（梯度）。

令人惊奇的是，经过推导（这里利用了Sigmoid函数的导数 `σ’(z) = σ(z)(1-σ(z))`），梯度公式变得异常简洁：

**对于参数 `w_j`：**
`∂J/∂w_j = (1/m) * Σ ( (ŷ⁽ⁱ⁾ - y⁽ⁱ⁾) * x_j⁽ⁱ⁾ )`

**对于参数 `b`：**
`∂J/∂b = (1/m) * Σ (ŷ⁽ⁱ⁾ - y⁽ⁱ⁾)`

**这个结果非常优美且重要！**
*   梯度是**所有样本的预测误差 `(ŷ - y)` 乘以对应特征值 `x_j` 的平均值**。
*   预测误差越大，梯度就越大，参数更新的步长也就越大。
*   这个形式与线性回归的梯度在形式上惊人地一致，尽管它们来自完全不同的代价函数。

**参数更新：**
在得到梯度后，我们按照以下规则更新参数：
`w_j = w_j - α * (∂J/∂w_j)`
`b = b - α * (∂J/∂b)`
其中 `α` 是学习率。

---

### 总结

逻辑回归的代价函数（交叉熵损失）是其成功的关键，因为它：

1.  **完美契合分类任务**：通过 `-log(ŷ)` 的形式，对“ confidently wrong”的预测施加巨大的惩罚。
2.  **保证可优化性**：它是一个凸函数，确保梯度下降能找到全局最优解。
3.  **推导出简洁的梯度**：使得参数更新的计算非常高效，形式清晰直观 `(ŷ - y)x_j`。

理解这个代价函数，就等于理解了逻辑回归模型是如何从数据中“学习”到那些关键的权重 `w` 和偏置 `b` 的。