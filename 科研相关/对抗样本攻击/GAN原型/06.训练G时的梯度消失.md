这个“大坑”是你从看论文到亲手写代码时最容易遇到的障碍，也是深度学习中关于“梯度消失”（Gradient Vanishing）的一个经典案例。

我们刚刚已经把 **公式 1** 里的 G 的目标理清楚了：

**理论目标（Original Objective）**：
$$ \min_G [\log(1 - D(G(z)))] $$

这就好比说：G 的目标是**最小化**“被警察抓住的概率”。

---

### 问题：为什么理论上的这个公式在实践中很难训练？

#### 1. 训练初期的状态
想象一下训练刚开始的时候（第一轮迭代）：
*   **G 很弱**：还没学会画画，生成的全是随机噪点图（乱码）。
*   **D 很强**：就算没怎么训练，也很容易区分“真实的清晰照片”和“乱码图”。
*   **结果**：D 会极其自信地把假图识别出来。
    *   也就是 $D(G(z)) \approx 0$。
    *   这意为着 D 认为它是假图的概率接近 100%。

#### 2. 数学灾难：梯度消失
这时候我们来看看 G 的 Loss 函数曲线。
我们要优化的函数是 $f(x) = \log(1 - x)$，这里的 $x$ 就是 $D(G(z))$。

*   大家知道，梯度就是函数的**斜率**。训练神经网络靠的是**大梯度**（斜率陡峭），这样参数更新才快。
*   **看图像**：你可以想象一下 $\log(1-x)$ 在 $x=0$ 附近的形状。
    *   在 $x=0$ 处（即 D 很容易识破假图时），切线的**斜率非常平缓**。
    *   你可以求个导试试：$\frac{d}{dx} \log(1-x) = \frac{-1}{1-x}$。
    *   当 $x \approx 0$ 时，导数大小约为 -1。
    *   这听起来还可以？但在深度神经网络中，D 如果非常自信（比如 $D(G(z)) = 0.00000001$），经过 Sigmoid 激活函数反向传播回去，这一点的梯度在传递多层后会变得微乎其微，导致 G **根本不知道该往哪个方向改**才能骗过 D。G 就会“躺平”，参数不更新了。

这就是论文中说的 **"Saturating Gradients"（梯度饱和）**。D 太强了，强到把 Loss 函数挤到了一个平坦的区域，G 感受不到改进的动力。

---

### 解决方案：换个脑子想问题

Goodfellow 提出，既然“最小化被抓概率”不好算，那我们换个等价的说法：

**实践目标（Heuristic Objective）**：
$$ \max_G [\log(D(G(z)))] $$

这好比说：G 的目标改为**最大化**“被误判为好人的概率”。

#### 这样做的好处：
我们要优化的函数变成了 $g(x) = \log(x)$。
*   同样在训练初期，G 很烂，D 很强，$D(G(z)) \approx 0$（也就是 $x \approx 0$）。
*   **看图像**：看看 $\log(x)$ 在 $x=0$ 附近的形状。
    *   在 $x$ 接近 0 的地方，$\log(x)$ 的曲线是**陡然下降的**（想想 log 图像左边那条垂直向下的尾巴）。
    *   求个导：$\frac{d}{dx} \log(x) = \frac{1}{x}$。
    *   当 $x \to 0$ 时（比如 $0.00001$），导数 $\to \infty$（无穷大）！
*   **结果**：这时候梯度**非常大**！这意味着当 G 很笨、D 很自信的时候，Loss 函数会给 G 一个**极强的惩罚信号**：“你太烂了！赶紧改！往死里改！”

### 总结“坑”与“填坑”

1.  **坑**：如果你老老实实按公式 1 写代码 `loss = min log(1 - D(G(z)))`，在最开始 D 很容易赢 G，导致 Loss 曲线平坦，梯度很小，G 学得极慢甚至不学了。
2.  **填坑（Trick）**：你在写代码时，要把 G 的 Loss 写成 `loss = max log(D(G(z)))`（在 PyTorch 里就是最小化 `-log(D(G(z)))`，这正好是 Binary Cross Entropy Loss 的一部分）。
3.  **效果**：逻辑上这两句话意思是差不多的（都是想骗过 D），但在**动力学（Dynamics）**上，第二种写法能在 G 表现最差的时候提供最大的梯度，逼着 G 快速进步。

这就好比教育学生：
*   方法一（理论）：考不到 100 分就有一点点不开心。（学生考 0 分时感觉无所谓，没动力）
*   方法二（实践）：考 0 分就狠狠打屁股。（学生考 0 分时痛感强烈，拼命去学）

这就是这篇论文中最经典的工程智慧之一。