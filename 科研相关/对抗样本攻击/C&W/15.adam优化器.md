### 深入剖析 Adam (Adaptive Moment Estimation)

作为一名科研新手，深入理解 Adam 是非常有价值的，因为它是目前深度学习中最流行的优化器（几乎是“默认选项”）。在 C&W 攻击中，它的作用更是不可替代。

我们就从最简单的 **SGD** 开始，一步步进化成 **Adam**，这样你就懂它的机理了。

---

### Phase 1: 朴素的 SGD (最笨的小人)

想象你在漆黑的山上，想下山（找 Loss 最小点）。你只能感觉到脚下的坡度（梯度 $g$）。

*   **策略：** 只要觉得哪边低，就往哪迈一步。
*   **公式：** $\theta_{t+1} = \theta_t - \eta \cdot g_t$
    *   $\theta$: 参数位置
    *   $\eta$: 学习率（步长，比如 0.01）
    *   $g$: 当前梯度
*   **缺点：**
    1.  **怕震荡：** 如果像在一个狭长的峡谷里，SGD 会在两边墙壁之间来回撞（震荡），很难沿着谷底往下走。
    2.  **怕平原：** 到了平缓的地方（梯度 $g$ 很小），它走得慢吞吞，甚至卡住不动（Saddle Point）。

---

### Phase 2: 加点惯性 —— Momentum (带惯性的小钢球)

为了解决“震荡”和“慢”，我们引入物理里的动量。

*   **策略：** 我不仅仅看当前的坡度，还要参考**这之前的速度**。如果我之前一直往东跑，现在坡度虽然让我往西，我也要先刹车、再转向，不能瞬间回头。
*   **公式：**
    1.  $m_t = \beta \cdot m_{t-1} + (1-\beta) \cdot g_t$  （积累历史动量）
    2.  $\theta_{t+1} = \theta_t - \eta \cdot m_t$
*   **好处：**
    *   在峡谷里震荡时，左右方向的动量会被抵消，沿着谷底方向的动量会累积，所以能更快通过。
    *   这就叫 **一阶矩估计 (First Moment Estimation)**。Adam 的名字里 "Moment" 就源于此。

---

### Phase 3: 自适应步长 —— RMSProp (聪明的探险家)

SGD 和 Momentum 都有个硬伤：**所有参数共用同一个学习率 $\eta$**。
但图片的像素有的变化剧烈（头发边缘），有的变化迟钝（背景天空）。我们希望：
*   对变化剧烈的参数，步子小一点（求稳）。
*   对变化迟钝的参数，步子大一点（求快）。

*   **策略：** 统计一下过去梯度的平方和（也就是梯度的量级）。梯度大的除以大数，梯度小的除以小数。
*   **公式：**
    1.  $v_t = \beta \cdot v_{t-1} + (1-\beta) \cdot g_t^2$ （积累梯度的平方，即二阶矩）
    2.  $\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{v_t} + \epsilon} \cdot g_t$
*   **好处：** 自动调节每个参数的刹车和油门。这就叫 **二阶矩估计 (Second Moment Estimation)**。

---

### Phase 4: 合体进化 —— Adam (全能战士)

**Adam = Momentum + RMSProp**
它既有惯性（冲过平原），又能自适应步长（稳定下坡）。

#### 核心算法流程：

1.  **计算梯度：** $g_t$
2.  **更新一阶矩（惯性）：**
    $m_t = \beta_1 m_{t-1} + (1-\beta_1) g_t$
    *(通常 $\beta_1 \approx 0.9$)*
3.  **更新二阶矩（自适应标尺）：**
    $v_t = \beta_2 v_{t-1} + (1-\beta_2) g_t^2$
    *(通常 $\beta_2 \approx 0.999$)*
4.  **偏差修正 (Bias Correction)：**
    *(这是 Adam 独有的细节。因为 $m$ and $v$ 初始化为0，刚开始会偏向0，所以要放大一点修正回来)*
    $\hat{m}_t = m_t / (1-\beta_1^t)$
    $\hat{v}_t = v_t / (1-\beta_2^t)$
5.  **最终更新：**
    $\theta_{t+1} = \theta_t - \eta \cdot \frac{\hat{m}_t}{\sqrt{\hat{v}_t} + \epsilon}$

---

### 为什么在 C&W 的 $\tanh$ 变换后 Adam 这么重要？

回到 C&W 攻击的场景：
$$ x' = \frac{1}{2}(\tanh(w) + 1) $$

请注意 **$\tanh(w)$ 的导数**是 $1 - \tanh^2(w)$。

*   **当 $w \approx 0$ 时：** 导数 $\approx 1$。梯度正常。
*   **当 $w$ 很大时（比如 $w=5$）：** $\tanh(5) \approx 0.9999$。
    *   导数 $\approx 1 - 0.9999^2 \approx 0.0002$。
    *   **梯度极小！**

**如果是 SGD：** 遇到 0.0002 的梯度，它更新量就是 $0.01 \times 0.0002 = 0.000002$。这就等于**停机了**。优化器根本推不动 $w$ 回来。

**如果是 Adam：**
1.  它看到梯度 $g$ 很小（0.0002）。
2.  它的分母 $\sqrt{\hat{v}_t}$ 也会记住这个梯度很小（接近 0.0002）。
3.  在计算更新量时：$\frac{\text{小梯度}}{\text{小分母}}$。
4.  分子分母一抵消，更新量依然能保持在一个合理的量级（比如 0.01）。
5.  **结果：** 即使 $w$ 跑到了梯度消失的平原区，Adam 依然能大步流星地把它拉回来，继续寻找最优解。

**这就是为什么 C&W 论文里说 SGD 经常 fails to find adversarial examples，而 Adam 却可以极其稳定地攻破模型的原因。**