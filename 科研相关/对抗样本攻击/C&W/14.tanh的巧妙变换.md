没问题！“变量代换”这一步（Change of Variables）是 C&W 攻击中最具工程师智慧的亮点之一。而 Adam 则是让整个魔法能够运转的引擎。

我们分两块来拆解：

---

### 第一部分：第二步 - 处理像素约束 (变量代换)

#### 1. 为什么会有这个烦恼？
我们要生成的对抗样本 $x + \delta$，不管再加上多少噪声，它本质上必须是一张**合法的图片**。
在数字图像处理中，像素值通常被归一化到 **[0, 1]** 之间（或者是 0-255，这里按 0-1 算）。

如果优化器很“狂野”，给你算出了一个像素值是 **1.5** 或者 **-0.2**，这在计算机看来就是非法数据（不可能有比全黑还黑、比全白还白的颜色）。

#### 2. 常规做法：Clipping（简单粗暴但有坑）
这就好比你在开车（优化），但路两边有护栏（0和1）。
常规做法是：你就闭着眼开，撞到护栏我就把你强行掰回来。

*   **操作：** 每次算完梯度更新 $x'$ 后，执行 `x' = min(max(x', 0), 1)`。
*   **后果（梯度消失）：**
    *   假设现在的像素是 1.0（已经顶到白色的极限了）。
    *   优化器算出来的梯度告诉你：*“为了攻击成功，这个像素还需要再亮一点，变成 1.2 最好。”*
    *   但是你把它 Clip 回 1.0 了。
    *   下一轮，优化器一看：*“咦？我让你变 1.2，你怎么还是 1.0？这里的梯度怎么完全不起作用？”*
    *   这就导致梯度在边界处**失效**，优化器会卡住，以为没路走了。

#### 3. C&W 的妙招：变量代换 (Change of Variables)

作者心想：*“既然带着镣铐（0-1 约束）跳舞这么难受，那我干脆换个舞台跳舞。”*

他引入了一个新的变量 **$w$**。
*   $w$ 是一个实数，它可以是 $-\infty$ 到 $+\infty$ 的任何值。
*   **完全自由！** 优化器想把 $w$ 推到 10000 还是 -500 都可以，没有任何护栏。

但是，我们需要把这个自由的 $w$ 变回一张合法的图 $x'$。于是作者找来了一个数学工具：**tanh (双曲正切函数)**。

*   **tanh 的魔法：**
    *   不管输入是什么（$-\infty$ 到 $+\infty$），`tanh(input)` 的输出永远被压缩在 **(-1, 1)** 之间。

*   **公式推导：**
    为了把 (-1, 1) 变成我们要的 (0, 1)，做一点简单的算术：
    1.  `tanh(w)` -> 范围 (-1, 1)
    2.  `tanh(w) + 1` -> 范围 (0, 2)
    3.  `(tanh(w) + 1) / 2` -> 范围 **(0, 1)**

    所以：
    $$ x' = \frac{1}{2}(\tanh(w) + 1) $$

*   **现在的优化过程：**
    1.  我们不再直接优化图像 $x'$，而是优化那个幕后的变量 $w$。
    2.  优化器算出梯度，要把 $w$ 从 10 增加到 20？**没问题，随便加！**
    3.  但在算 Loss 的时候，我们会先把 $w$ 映射成合法的图像 $x'$。
    4.  $w=10$ 时，$\tanh(10) \approx 0.99999$，图像像素接近 1。
    5.  $w=20$ 时，$\tanh(20) \approx 1.0$（更接近1），图像像素还是合法的。

**总结：** 这种变换让优化器可以在一个**无约束的广阔空间**里奔跑，而映射回来的结果又永远是**合法且平滑**的。这解决了边界处梯度卡死的问题。

---

### 第二部分：关于 Adam 优化器

你知道 SGD（随机梯度下降），你可以把 Adam (Adaptive Moment Estimation) 理解为 **“加上了自动变速箱和路况适应系统的 SGD”**。

在 C&W 攻击中，作者明确指出：**使用 Adam 也是成功的关键之一。**

#### 1. 为什么 SGD 不够好？
*   **学习率（Step Size）难调：** 对于 C&W 这种复杂的非凸优化问题，梯度在不同方向上差异巨大。有的方向很陡（梯度大），有的方向很平（梯度小）。
    *   如果用 SGD 给定一个固定的学习率：有的地方走太慢，有的地方又步子太大震荡。
*   **容易陷坑：** SDG 像个笨重的小球，容易卡在马鞍面（Saddle Point）或者局部极小值里出不来。

#### 2. Adam 强在哪？（简单理解）
Adam 结合了 **动量（Momentum）** 和 **自适应学习率（Adaptive Learning Rate）**。

*   **动量 (Momentum):**
    *   这就好比小球下山有了**惯性**。如果之前的步伐一直往东走，哪怕现在的梯度稍微有点偏北，惯性也会带着球继续往东冲一段。
    *   这帮它**冲过陡峭的震荡区**，或者**冲出浅浅的局部坑**。

*   **自适应学习率 (RMSProp 的思想):**
    *   Adam 会给每个参数（比如图片的每个像素对应的 $w$）单独维护一套学习率。
    *   如果某个像素梯度一直很大，Adam 会说：“这坡太陡了，步子跨小点，别摔倒。”
    *   如果某个像素梯度一直很小，Adam 会说：“这地儿太平了，步子跨大点，赶紧走。”

#### 3. 为什么 C&W 一定要用 Adam？
因为前面的 **tanh 变换**带来了一个副作用：
*   在 $w$ 很大或很小的时候（饱和区），$\tanh$ 的梯度会变得非常非常小。
*   如果是普通的 SGD，遇到这么小的梯度几乎就不动了。
*   但是 **Adam 能适应这种情况**，它发现梯度变小了，就会自动尝试放大更新步伐（Scaling），从而保证优化能继续进行，直到找到真正的最优解。

**一句话总结这一节：**
作者用 **tanh 变换**消除了边界约束，创造了一个平滑的无约束空间；然后用 **Adam** 这个强力引擎，在这个空间里自动调节速度和方向，从而高效、精准地找到了那个能骗过模型的最小扰动。