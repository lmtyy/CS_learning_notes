好，我们进入论文的核心技术区：**Section 3 (Method)**。这一节是整篇论文的灵魂，也是你复现代码时要一比一还原的部分。

作者把这部分分成了三块来讲：
1.  **DiffPure (3.1)**: 怎么做净化？（核心算法）
2.  **Theoretical Analysis**: 为什么通过扩撒能洗掉攻击？
3.  **Adjoint Method (3.2)**: 遇到显存爆炸怎么办？

---

### 第一块：DiffPure 怎么玩？ (Section 3.1)

作者的思路简单又粗暴：**既然扩散过程能把所有数据都毁成噪声，那它肯定顺便也能毁掉对抗扰动。**

#### 步骤一：前向扩散 (Forward SDE) —— 主动加噪
给定一张被攻击的图 $x_a$（Adversarial Example），我们不直接去分类它，而是：
*   **动作**：把它作为起点 $x(0)$，沿着前向 SDE 跑一段路，直到时间 $t=t^*$。
*   **公式 (Eq. 3)**：
    $$ x(t^*) = \sqrt{\alpha(t^*)} x_a + \sqrt{1 - \alpha(t^*)} \epsilon $$
    这就是你无比熟悉的 DDPM 加噪公式！
    *   $x_a$：对抗样本。
    *   $\epsilon$：随机采样的高斯噪声。
    *   $t^*$：这就是那个关键的超参数（Hyperparameter）。**这步不需要跑循环**，因为高斯过程可以直接一步采样算出来。**快！**
*   **目的**：此时的 $x(t^*)$ 已经是个半噪声图了，此时对抗扰动（那些精心设计的高频纹理）已经被强力的高斯噪声打乱、稀释了。

#### 步骤二：反向去噪 (Reverse SDE) —— 重建图像
拿到了半噪声图 $x(t^*)$，现在的任务是把它变回干净图。
*   **动作**：把 $x(t^*)$ 当作起始点，沿着反向 SDE 往回走，走回到 $t=0$。
*   **公式 (Eq. 4)**：
    $$ \hat{x}(0) = \text{sdeint}(x(t^*), f_{rev}, g_{rev}, \bar{w}, t^*, 0) $$
    这个 `sdeint` 就是一个黑盒求解器（Solver），你可以把它理解为一个 `for` 循环，从 $t^*$ 倒着算回 0。
    *   **核心组件**：里面用到了 $s_\theta(x, t)$，也就是预训练好的 Score Network（或 UNet），它知道干净图片的分布长啥样。
*   **结果**：因为 $s_\theta$ 只学过干净图片的分布，所以它会强行把 $x(t^*)$ 拉回到**干净数据流形**上。即便 $x_a$ 里藏了猫腻，因为在加噪阶段被打乱了，现在去噪阶段模型只会根据剩余的宏观语义（比如有个猫的轮廓）把猫复原出来，而不会把攻击噪声复原出来。

---

### 第二块：理论分析 —— 为什么要选 $t^*$？

这一部分作者给了两个非常漂亮的定理（这是发 ICML 的加分项），你需要理解它们的直觉：

**Theorem 3.1: 攻击消失定理**
*   **结论**：随着扩散时间 $t$ 的增加，**“干净数据分布”和“对抗数据分布”之间的距离（KL散度）会越来越小。**
*   **人话**：你加的噪越多，对抗样本和普通样本就越分不清，它们最后都会变成一样的高斯噪声。这证明了 Forward Process 是有效的清洗手段。

**Theorem 3.2: 语义保留定理**
*   **结论**：净化后的图 $\hat{x}(0)$ 和原版干净图 $x$ 之间的差距，虽然主要取决于初始对抗扰动，但也和 $t^*$ 有关。如果 $t^*$ 太大，恢复误差就会变大。
*   **人话**：你不能一直加噪。如果你把图彻底变成了纯噪声 ($t=1$)，那神仙也就救不回来原来的语义了。

**总结**：存在一个 **Sweet Spot**（最佳点）$t^*$。在复现时，这是一个必须调节的参数（通常 $t^* \in [0.1, 0.3]$）。

---

### 第三块：Adjoint Method (Section 3.2) —— 你的噩梦也是你的武器

这一块是为了解决**怎么评估**的问题。
如果我想攻击你的 DiffPure 系统，我需要计算：
$$ \nabla_{x_{adv}} \text{Classifier}(\text{DiffPure}(x_{adv})) $$
这意味着梯度要穿过 Classifier，还要穿过 DiffPure 里面的 `sdeint` 过程。

#### 问题：O(N) 显存爆炸
如果你把 DiffPure 看作一个几百层的 ResNet（因为要迭代去噪几百步），那存下每一层的中间 Feature 来做 Backprop，显存（Memory）是线性增长的。对于大图（ImageNet），这是不可接受的。

#### 解决：O(1) 伴随状态法 (Adjoint Method)
作者引入了数学魔法 **Equation 6**。
*   简单说：我不存中间状态了。
*   当我需要求导时，我**构造另一个新的 SDE（Augmented SDE）**。
*   这个新 SDE 的状态包含了 [原图, 梯度] 两个部分。
*   我只需要倒着再跑一遍这个新 SDE，就能把梯度解出来。
*   **代价**：时间变多了（多跑了一遍 SDE）。
*   **收益**：显存变成常数级 $O(1)$ 了！不管你迭代多少步，显存都只占那一丢丢。

**注意：**
这部分代码实现比较难写。但好消息是，作者使用了 **`torchsde`** 这个库，这个库已经把 Adjoint Method 封装好了。你在复现时，只需要调用 `torchsde.sdeint_adjoint` 这个函数，而不是普通的 `torchsde.sdeint`，问题就解决了。

---

### Section 3 总结 & 复现指引

读完这一节，你的代码框架应该出来了：

```python
# 伪代码逻辑

# 1. 准备预训练好的Score模型
score_model = load_pretrained_score_model()

# 2. 定义 SDE (Forward & Reverse)
# 这些都在 torchsde 文档里有现成的，按照 Eq.5 把 drift 和 diffusion 填进去
sde = VPSDE(score_model)

def diffpure(x_adv, t_star):
    # Step 1: Forward (加噪) - Eq. 3
    # 直接采样，不用 SDE solver，快！
    noise = torch.randn_like(x_adv)
    mean, std = sde.marginal_prob(x_adv, t_star)
    x_diffused = mean + std * noise
    
    # Step 2: Reverse (去噪) - Eq. 4
    # 这里用 solver 解回去
    # 如果只是推理，用 sdeint；如果要做白盒攻击，用 sdeint_adjoint
    x_purified = sdeint(sde, x_diffused, t_star, 0) 
    
    return x_purified

# 3. 接入分类器
logits = classifier(diffpure(x_adv, t_star=0.2))
```

这就是这篇论文的核心方法论。是不是感觉也没那么复杂？本质就是：**加点噪打碎攻击，再用强力生成模型重绘回来，如果为了评测，记得开省显存模式。**