没问题，节奏由你把控。我们进入 **Section 2: Background**。

这一部分是作者在正式抛出自己方法前的铺垫，主要介绍了两个核心概念：**Reverse Process（逆向/去噪过程）** 和 **Forward Process（前向/加噪过程）**。这部分是全篇最基础的定义，一定要看懂符号。

---

### 第一小节：宏观定义 (What is Diffusion Model?)

> *A diffusion probabilistic model is a parameterized Markov chain trained using variational inference to produce samples matching the data after finite time.*

这句话里有两个关键词：
1.  **Parameterized Markov chain (参数化马尔可夫链)**：
    *   **马尔可夫链**：意思是“现在的状态只取决于上一刻的状态，跟以前没关系”。即 $x_{t-1}$ 只看 $x_t$，不看 $x_{t+1}$。
    *   **参数化**：指这个链条里有一个神经网络 $\theta$ 在控制。

2.  **Reverse Process (逆向过程 / 生成过程, Eq. 1)**：
    $$ p_\theta(x_{0:T}) := p(x_T) \prod_{t=1}^T p_\theta(x_{t-1}|x_t) $$
    $$ p_\theta(x_{t-1}|x_t) := \mathcal{N}(x_{t-1}; \mu_\theta(x_t, t), \Sigma_\theta(x_t, t)) $$

    *   **解读**：这就是我们上一节说的“从模型抽样”。
        *   从纯噪声 $p(x_T) = \mathcal{N}(0, I)$ 开始。
        *   一步步乘起来（$\prod$），这就是那个 Loop 循环。
        *   每一步都在算一个高斯分布 $\mathcal{N}$，也就是确定**均值 $\mu_\theta$** 和 **方差 $\Sigma_\theta$**。
    *   **重点**：这里的 $\mu_\theta$ 是个函数（神经网络），输入是 $(x_t, t)$，这就是你要训练的 U-Net。

---

### 第二小节：前向过程 (The Forward Process)

这里作者定义了“数据是怎么变脏的”。

> *...defined by a fixed Markov chain that gradually adds Gaussian noise to the data...*

1.  **Forward Process (前向过程, Eq. 2)**：
    $$ q(x_{1:T}|x_0) := \prod_{t=1}^T q(x_t|x_{t-1}) $$
    $$ q(x_t|x_{t-1}) := \mathcal{N}(x_t; \sqrt{1-\beta_t}x_{t-1}, \beta_t I) $$

    *   **解读**：
        *   注意这里用的是 $q$，不是 $p$。$q$ 代表**真实发生的物理过程**，不需要学习。
        *   **Fixed (固定的)**：这里没有参数 $\theta$。怎么加噪是人定死的规则，不是网络学的。
        *   ** $\beta_t$ **：刚才说过的，每次加噪的步长（Variance schedule）。作者设定它是从 $10^{-4}$ 到 $0.02$ 的常数。
        *   公式意思：现在的图 = 上一刻的图乘个系数 + 一个噪声。

2.  **神技：任意时刻采样 (Eq. 4)**
    作者在这里放出了那个极其重要的公式：
    $$ q(x_t|x_0) = \mathcal{N}(x_t; \sqrt{\bar{\alpha}_t}x_0, (1 - \bar{\alpha}_t)I) $$

    *   **科研价值**：这是做实验、写代码、搞攻击的基础。有了它，你不需要写 `for` 循环就能直接造出第 500 步的攻击目标图。
    *   **推导**：这是把上面那个 Eq. 2 迭代代入推出来的，利用了高斯分布加高斯分布还是高斯分布的性质。

---

### 第三小节：训练目标 (Training Logic)

这一节最后抛出了那个原始的 Loss Function。

1.  **最大化对数似然下界 (Eq. 3)**：
    $$ \mathbb{E}[-\log p_\theta(x_0)] \le \mathbb{E}_q [ -\log p(x_T) - \sum_{t \ge 1} \log \frac{p_\theta(x_{t-1}|x_t)}{q(x_t|x_{t-1})} ] = L $$

    *   这是传统的 **Variational Lower Bound (ELBO)**。
    *   看不懂没关系，你只需要知道：**我们本来应该优化这个 $L$，但它太难看了，所以后面把它拆了。**

2.  **Beta 调度 (Variance Schedule)**
    *   作者提了一嘴：我们选择把 $\beta_t$ 设为常数。虽然有人尝试把 $\beta_t$ 也设成可学习的参数，但这篇论文里为了简单，固定住了。
    *   *攻击视角*：因为 $\beta$ 是固定的，这实际上暴露了模型的 dynamics。如果攻击者知道这套 $\beta$ schedule，就相当于拿到了地图。

---

### Section 2 总结（看完这段需要记住什么？）

这一章就是定义游戏规则：

1.  **规则一（生成）**：从 $x_T$ 倒着推回 $x_0$，每一步也是高斯分布，这是神经网络要学的 ($p_\theta$)。
2.  **规则二（加噪）**：从 $x_0$ 正着推到 $x_T$，是固定规则，不需要学 ($q$)。
3.  **规则三（捷径）**：我们可以跳步骤直接算出任意时刻的 $x_t$（Eq. 4）。

只要记住这三点，你也就可以进入核心的 Section 3 也就是“怎么改 Loss”的部分了。