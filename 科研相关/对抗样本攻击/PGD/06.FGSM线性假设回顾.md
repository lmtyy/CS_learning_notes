你这是一个非常敏锐且精准的问题！能问出这个问题，说明你当年看 FGSM 论文（*Explaining and Harnessing Adversarial Examples, 2015*）时不仅仅是看了代码，而是真的思考了背后的原理。

你是对的：**Goodfellow 当年提出的“线性假设”（Linearity Hypothesis），其核心论点讨论的确实是“模型架构/高维空间”的性质，而不仅是“损失函数”。**

但这里有一个微妙的逻辑链条，从“模型的线性”推导到了“损失函数的局部线性近似”。让我帮你把这根线重新理清楚，你就明白为什么 Madry 这篇 PGD 论文会说 FGSM 是一种“线性化方法”了。

---

### 一、 回顾 Goodfellow 的“线性假设” (The Linearity Hypothesis)

在 2014 年之前，大家普遍认为神经网络之所以会被对抗样本攻击，是因为它们**太非线性了**，导致在高维空间里这就这出了很多奇怪的“孤岛”或“盲区”。

但 Goodfellow 在 FGSM 论文里颠覆了这个观点。他说：**不对，神经网络之所以脆弱，恰恰是因为它们在高维空间里表现得“太线性”了。**

#### 这里的“线性”指什么？
指的是**激活值（Activation）与输入（Input）的关系**。

*   **例子**：考虑一个神经元 $output = w^T x$（即便加了 ReLU，也是分段线性的）。
*   **推导**：
    假设我们将输入 $x$ 改变了一点点 $\eta$，其中 $\eta$ 的每个元素都很小（$|\eta| < \epsilon$）。
    新的输出变成了：
    $$ w^T (x + \eta) = w^T x + w^T \eta $$
    这多出来的一项 $w^T \eta$ 是什么呢？
    如果 $w$ 是一个高维向量（比如 $n$ 维），而我们要让这一项最大，我们会让 $\eta$ 的符号和 $w$ 一致（即 $\eta = \text{sign}(w)$）。
    那么，$w^T \eta$ 的最大值就是 $\epsilon \cdot n \cdot m$（$m$ 是 $w$ 的平均幅值）。

*   **结论**：哪怕 $\epsilon$ 很小，只要维度 $n$ 很大（高维空间），**这种线性的累积效应**就会像滚雪球一样，导致输出值发生巨大的变化。
    > **Goodfellow 的原话**：*"Adversarial examples can be explained as a property of high-dimensional dot products."*

所以你是对的，这里的线性假设最初是在解释**“为什么脆弱”**（因为模型在高维空间的线性行为导致了误差的线性放大）。

---

### 二、 从“模型线性”到“损失函数线性近似”

既然模型内部的行为主要是由线性累积主导的，Goodfellow 紧接着提出了攻击方法（FGSM）。他的逻辑是：**既然模型是线性的，那我们就假设损失函数 $J(\theta, x, y)$ 在局部也是线性的。**

这就是 FGSM 算法的数学来源——**一阶泰勒展开（Taylor Expansion）**。
$$ J(\theta, x+\eta, y) \approx J(\theta, x, y) + \eta^T \cdot \nabla_x J(\theta, x, y) $$

在这个公式里：
*   $J(\theta, x, y)$ 是常数（当前损失）。
*   $\nabla_x J(\theta, x, y)$ 是常向量（当前梯度）。
*   这个近似公式把复杂的 Loss 曲面看作了一个**平整的斜坡（Planar Surface）**。

为了最大化这个近似后的损失函数（走完这个线性斜坡），你需要做的就是让 $\eta$ 的方向完全在这个斜坡的上升方向上。
$$ \eta = \epsilon \cdot \text{sign}(\nabla_x J) $$
这就是 **Fast Gradient Sign Method (FGSM)**。

---

### 三、 Madry 的 PGD 论文是在“反对”什么？

现在回到你正在看的这篇 PGD 论文。

Madry 并没有否认 Goodfellow 关于“模型高维线性导致脆弱”的理论，但他挑战了**“用线性近似来攻击”的有效性**。

1.  **FGSM 的视角**：它认为由于模型是线性的，所以损失函数 $J$ 就像一个平滑的、一直向上的**大斜坡**。你只要沿着梯度方向迈一大步（一步到位），就能到达最高点。

2.  **Madry (PGD) 的发现**：
    *   Madry 在 **Section 3.1** 里说：兄弟，我做实验看了，损失函数 $J$ 实际上并不是一个光滑的大斜坡。
    *   它其实是像波浪一样，有很多弯曲、褶皱和局部极值。
    *   **结论**：你用 FGSM 那个“线性假设”迈一大步，很可能直接**跨过了**真正的最高点，或者撞到了旁边的墙上，而不是精准地停在最高点。

### 总结你的疑惑

你的记忆是完全正确的：
*   **Goodfellow 的“线性假设”**：确实是指**模型架构在高维空间表现出的线性性质**是导致脆弱性的根源。
*   **但这导致了 FGSM 的攻击手段**：采用**损失函数的线性化（一阶导数）**来估算最坏扰动。

Madry 的这篇论文则是在说：**虽然你的线性归因很有道理，但你在攻击时只做“一步线性近似”太粗糙了，为了找到那个 Min-Max 里的 Max，我们必须老老实实地走多步（PGD），去探索那个非线性的损失曲面。**