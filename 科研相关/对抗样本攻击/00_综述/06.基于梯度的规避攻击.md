先把“梯度”用最直观的方法复习一下，再用这个直觉去理解“基于梯度的规避攻击”。

一、梯度到底是什么（直觉版）
- 1 维情况（只有一个自变量 x）：导数就是“斜率”。在某点，导数为正表示往右走函数升高，为负表示往右走函数降低；绝对值越大，变化越快。
- 多维情况（x 是向量）：梯度就是“所有变量的斜率组成的向量”。它有两个关键信息：
  - 方向：指向函数“上升最快”的方向（最陡的上坡）。
  - 长度：表示在该点沿这个方向，函数上升得有多快（有多陡）。
- 一个小比喻：把函数看成一座山，当前位置是一点。梯度的方向就是指南针指向“最陡上坡”的方向；如果你要尽快爬高，就沿梯度方向走；如果要尽快下山，就沿反方向走。

二、把梯度放进深度学习里（和训练、攻击的关系）
- 模型训练时，我们有损失函数 L(参数、输入、标签)。训练是对“参数”的梯度下降：调整参数，降低损失。
- 攻击时，我们对“输入”的梯度动手：计算损失对输入的梯度 ∇x L，这个向量告诉我们“怎样改输入，能最快让损失变大或变小”。
  - 想让模型判错（非定向攻击）：沿“让损失变大”的方向改输入（往梯度方向走）。
  - 想让模型变成某个目标类 t（定向攻击）：让“针对目标类的损失变小”（往相反方向走）。

三、基于梯度的规避攻击在做什么（一步步）
目标（两种常见）：
- 非定向：让模型不再预测为原来的正确标签（只要错就行）。
- 定向：把样本变成模型指定的目标类别（比如把“猫”骗成“狗”）。

约束（必须满足）：
- 改动要小（不容易被人或系统察觉），常用“改动大小”的限制，比如每个像素最多改动 ε（L∞ 约束）或整体改动的 L2 范数不超过阈值。
- 在安全场景里，还要“保持功能/格式”不被破坏（例如可执行程序还能运行、PDF 能打开、网络包协议合法）。

常见三种梯度类攻击（从易到强）：
1) FGSM（Fast Gradient Sign Method，单步）
- 思路：在当前点，朝着让损失上升最快的方向走“一小步”。
- 操作（直白说）：x_adv = x + ε · sign(∇x L)
  - sign 只取方向（+1/-1），一步到位；ε 是步长（最大改动幅度）。
- 特点：实现简单、速度快，但攻击强度有限。

2) PGD（Projected Gradient Descent，多步迭代）
- 思路：不一次到位，而是“多次小步”沿梯度方向推进，每一步都“投影/裁剪回允许的改动范围”（比如保证每个像素改动不超过 ε，数值在[0,1]合法范围）。
- 伪流程：
  - 从原图（有时加一点随机初始扰动）开始；
  - 循环 T 次：算梯度 → 沿梯度方向走一步 → 裁剪到[0,1] → 再把整体扰动限制在“离原图不超过 ε”的球里；
  - 迭代后得到对抗样本。
- 特点：比 FGSM 强很多，是评估鲁棒性的常用基线。

3) CW（Carlini & Wagner，优化法）
- 思路：搭一个优化目标，把“尽量小的扰动”和“达到攻击目的”共同考虑，用数值优化慢慢找出“最小的可行扰动”。
- 特点：成功率高但更慢、实现/调参更复杂，常用于强评估。

为什么“梯度”能帮你骗过模型（直觉解释）
- 深度网络在“很小的局部”看起来近似线性；在高维空间里，哪怕每个维度只改一点点，加起来的综合效果也可能很大。梯度正好告诉你“综合最敏感方向”，沿着这个方向“轻轻推”，就更容易让模型犯错。

四、把公式翻成“人话例子”（以图像为例）
- 你有一张正确分类为“猫”的图片 x，模型对它的损失 L 很小。
- 你计算 ∇x L（损失对像素的敏感度），它告诉你“把哪些像素调亮/调暗能让损失长得最快”。
- FGSM：一次性把所有像素按这个方向轻微调一丁点（步长 ε），得到 x_adv。人眼几乎看不出差别，但模型可能已经判错。
- PGD：不一次性改到位，而是改一点、检查、再改一点，反复多次，效果更强。

五、在安全（离散/功能约束）场景怎么用“梯度”
- 难点：输入不是“平滑的像素”，而是“离散/二进制/结构化”，还要保持能运行/能解析。直接“加减一个单位”可能让文件/程序坏掉。
- 常见做法：
  - 先在“可微的特征向量”上用梯度找“哪些特征要改”（比如把某些 0/1 位改成 1）；再把这些改动翻译成实际可执行的修改（在 PDF 插入合法对象、在 PE 插入不影响功能的 API 调用、在流量中按协议合规地调整字段）。
  - 或者定义一个“可操作动作集合”（只能做追加/插入/重排等不破坏功能的动作），每一步沿“梯度指的方向”选择最接近的可执行动作，保持总改动受限。
- 重点：最后要验证“功能是否保持”（程序能运行、PDF 能打开、流量合法），否则只是“特征层面的成功”，不算真正实用攻击。

六、怎么选参数（图像任务的常见值，仅作感觉）
- L∞ 的 ε（数据归一化到[0,1]时）：
  - MNIST 常用 0.1–0.3
  - CIFAR-10 常用 8/255（≈0.0314）
- PGD：步数 T 10–40 步，步长 α ≈ ε/4 到 ε/10；每步后记得“裁剪+投影”。

七、评估时看什么
- 攻击成功率（非定向/定向）
- 扰动大小（是否在约束内）
- 是否保持功能（安全场景必须测）
- 黑盒场景下的查询次数/时间成本
- 对防御的自适应性（知道防御后还能绕过吗）

八、你可以马上做的一个小练习（帮助把梯度直觉落地）
- 在 CIFAR-10 上找一个训练好的模型：
  - 先跑 FGSM（ε=8/255）看看干净准确率和被攻击后的准确率变化；
  - 再跑 PGD（比如 20 步，步长 α=2/255）比较攻击强度；
  - 观察：为什么 PGD 更“狠”？调大/调小 ε 会发生什么？
- 如果需要，我可以把一份最小可运行的 PyTorch 脚本发给你（含 FGSM/PGD 两种攻击与评估），你直接运行就能看到梯度在“引导”你怎么改输入、模型如何被“轻推”后犯错。

如果你还想把这个思路迁移到安全数据（比如 PDF 或恶意软件特征），我可以再给你“特征级 PGD（只允许将 0→1 的若干位打开）”的简化模板，并教你如何把“改特征”翻译成“改真实样本”的具体步骤。